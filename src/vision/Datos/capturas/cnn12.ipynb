{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==2.7"
      ],
      "metadata": {
        "id": "ETclztbAhbQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "8b6Rgiue7_-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "ruta_archivo = '/content/drive/My Drive/cnn1ar/posesxyz.csv'\n",
        "datos = pd.read_csv(ruta_archivo)\n",
        "# print(datos.head)"
      ],
      "metadata": {
        "id": "Ft-tZpX38Plh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoded_data = pd.get_dummies(datos, columns=['theta_i'])\n",
        "# print(encoded_data.head())"
      ],
      "metadata": {
        "id": "jwjsPomu5IYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "AgFpy7o4ZJ4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow==2.10\n"
      ],
      "metadata": {
        "id": "QVTCsbP7ZZ9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "id": "qDCwK9Df_npH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertir valores de las columnas de imágenes a strings\n",
        "datos['ruta_imagen1'] = datos['ruta_imagen1'].astype(str)\n",
        "datos['ruta_imagen2'] = datos['ruta_imagen2'].astype(str)\n",
        "datos['ruta_imagen3'] = datos['ruta_imagen3'].astype(str)"
      ],
      "metadata": {
        "id": "fMMCTR5f-EMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_height, img_width = 240, 240\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "# datos\n"
      ],
      "metadata": {
        "id": "2KX4WmAXx_wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdir1 = '/content/drive/My Drive/cnn1ar/abv'\n",
        "df = pd.read_csv(ruta_archivo)\n",
        "# df"
      ],
      "metadata": {
        "id": "Ani8Hl3v-h6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un generador de imágenes para el conjunto de entrenamiento\n",
        "# Mezclar aleatoriamente los datos\n",
        "datos = datos.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento, validación y prueba\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, valtest_data = train_test_split(datos, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(valtest_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(len(train_data), len(val_data), len(test_data))\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.1  # Especificar la fracción de datos para validación\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=sdir1,\n",
        "    x_col=\"ruta_imagen1\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    subset=\"training\",  # No necesitas especificar \"subset\" aquí\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,  # Usar los mismos datos de entrenamiento para validación\n",
        "    directory=sdir1,\n",
        "    x_col=\"ruta_imagen1\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    subset=\"validation\",  # No necesitas especificar \"subset\" aquí\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    x_col=\"ruta_imagen1\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    directory=sdir1,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    shuffle=True\n",
        ")\n",
        "datagen2 = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.1  # Especificar la fracción de datos para validación\n",
        ")\n",
        "\n",
        "train_generator2 = datagen2.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=sdir1,\n",
        "    x_col=\"ruta_imagen2\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_generator2 = datagen2.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=sdir1,\n",
        "    x_col=\"ruta_imagen2\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_datagen2 = ImageDataGenerator(rescale=1./255)\n",
        "test_generator2 = test_datagen2.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    x_col=\"ruta_imagen2\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    directory=sdir1,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "datagen3 = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.1  # Especificar la fracción de datos para validación\n",
        ")\n",
        "\n",
        "train_generator3 = datagen3.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=sdir1,\n",
        "    x_col=\"ruta_imagen3\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_generator3 = datagen3.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=sdir1,\n",
        "    x_col=\"ruta_imagen3\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_datagen3 = ImageDataGenerator(rescale=1./255)\n",
        "test_generator3 = test_datagen3.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    x_col=\"ruta_imagen3\",\n",
        "    y_col=['t1', 't2', 't3', 't4', 't5', 't6'],\n",
        "    directory=sdir1,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\",\n",
        "    shuffle=True\n",
        ")\n",
        "# img_[8.63937979737193, -1.0471975511965979, -1.308996938995747, 175.38053912427813, 96.21302305938374, 41.496193992059844].png"
      ],
      "metadata": {
        "id": "Sc3zKpMlCkG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6, activation='linear'))\n",
        "\n"
      ],
      "metadata": {
        "id": "Vku-GoOPZjKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0000001)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n"
      ],
      "metadata": {
        "id": "UuoLHw5orhht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "# epochs = 30\n",
        "# history0 = model.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=val_generator,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1\n",
        "# )\n",
        "# history = model.fit(\n",
        "#     train_generator2,\n",
        "#     validation_data=val_generator2,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1\n",
        "# )\n",
        "# history1 = model.fit(\n",
        "#     train_generator3,\n",
        "#     validation_data=val_generator3,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# Crear generador combinado que incluye las tres rutas de imágenes y etiquetas\n",
        "class CombinedGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, generators):\n",
        "        self.generators = generators\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.generators[0])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = []\n",
        "        y = []\n",
        "        for generator in self.generators:\n",
        "            x_batch, y_batch = generator[index]\n",
        "            X.append(x_batch)\n",
        "            y.append(y_batch)\n",
        "        X = np.concatenate(X, axis=0)\n",
        "        y = np.concatenate(y, axis=0)\n",
        "        return X, y\n",
        "\n",
        "combined_generator = CombinedGenerator([train_generator, train_generator2, train_generator3])\n",
        "\n",
        "# Entrenar el modelo con el generador combinado\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(\n",
        "    combined_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "bINqaUrtr6W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(history0.history['loss'], label='train_loss')\n",
        "# plt.plot(history0.history['val_loss'], label='val_loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history0.history['mean_squared_error'], label='train_mse')\n",
        "# plt.plot(history0.history['val_mean_squared_error'], label='val_mse')\n",
        "# plt.title('Training and validation mean squared error')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Mean Squared Error')\n",
        "# plt.legend()\n",
        "#-----------------\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mean_squared_error'], label='train_mse')\n",
        "plt.plot(history.history['val_mean_squared_error'], label='val_mse')\n",
        "plt.title('Training and validation mean squared error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#----------------\n",
        "# plt.plot(history1.history['loss'], label='train_loss')\n",
        "# plt.plot(history1.history['val_loss'], label='val_loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history1.history['mean_squared_error'], label='train_mse')\n",
        "# plt.plot(history1.history['val_mean_squared_error'], label='val_mse')\n",
        "# plt.title('Training and validation mean squared error')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Mean Squared Error')\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "feo3pahbuDlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)\n"
      ],
      "metadata": {
        "id": "fdvq15U1uLst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print(val_loss)\n",
        "mae = history.history['mean_squared_error']\n",
        "val_mae = history.history['mean_squared_error']\n",
        "print(val_mae)"
      ],
      "metadata": {
        "id": "FTJhN_2DL9ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "rmse = np.sqrt(history.history['mean_squared_error'])\n",
        "val_rmse = np.sqrt(history.history['val_mean_squared_error'])\n",
        "print(len(val_rmse))\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "irzmFjw8McRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = model.layers[3]  # Obtener la capa de convolución número 3\n",
        "# get_weights()\n",
        "weights = conv_layer.get_weights()  # Obtener los pesos de la capa\n",
        "biases = conv_layer.get_weights()  # Obtener los sesgos de la capa\n",
        "print(\"Pesos de la capa de convolución 3:\", weights)\n",
        "print(\"Sesgos de la capa de convolución 3:\", biases)\n",
        "for layer in model.layers:\n",
        "    if hasattr(layer, 'weights'):\n",
        "        weights = layer.get_weights()\n",
        "        print(layer.name, weights)\n"
      ],
      "metadata": {
        "id": "zct8a9D6M_rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fHrYwLOcMgqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.applications.resnet import ResNet50\n",
        "# resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n"
      ],
      "metadata": {
        "id": "-26dZ8cgyKEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Activation, BatchNormalization, add, Flatten\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# def res_block(inputs, filters):\n",
        "#     x = Conv2D(filters, kernel_size=3, padding='same')(inputs)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Conv2D(filters, kernel_size=3, padding='same')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     shortcut = Conv2D(filters, kernel_size=1, padding='same')(inputs) # capa adicional para asegurar la forma de entrada y salida\n",
        "#     x = add([x, shortcut])\n",
        "#     x = Activation('relu')(x)\n",
        "#     return x\n",
        "\n",
        "\n",
        "# def create_model(input_shape):\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     x = Conv2D(128, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
        "#     x = res_block(x, filters=64)\n",
        "#     x = res_block(x, filters=64)\n",
        "#     x = res_block(x, filters=64)\n",
        "#     x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
        "#     x = res_block(x, filters=128)\n",
        "#     x = res_block(x, filters=128)\n",
        "#     x = res_block(x, filters=128)\n",
        "#     x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
        "#     x = res_block(x, filters=256)\n",
        "#     x = res_block(x, filters=256)\n",
        "#     x = res_block(x, filters=256)\n",
        "#     x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
        "#     x = res_block(x, filters=512)\n",
        "#     x = res_block(x, filters=512)\n",
        "#     x = res_block(x, filters=512)\n",
        "#     x = Flatten()(x)\n",
        "#     outputs = Dense(6, activation='linear')(x)\n",
        "#     model = Model(inputs=inputs, outputs=outputs)\n",
        "#     return model\n",
        "\n",
        "# model1 = create_model(input_shape=(224,224,3))\n",
        "# opt = Adam(learning_rate=0.0001)\n",
        "# model.compile(optimizer=opt, loss='mse', metrics=[keras.metrics.MeanSquaredError()])\n",
        "# model.summary()\n"
      ],
      "metadata": {
        "id": "46t6wJQKyOv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(train_generator, epochs=10, validation_data=val_generator)\n"
      ],
      "metadata": {
        "id": "axx9KL44182A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(history.history['loss'], label='train_loss')\n",
        "# plt.plot(history.history['val_loss'], label='val_loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['mean_squared_error'], label='train_mse')\n",
        "# plt.plot(history.history['val_mean_squared_error'], label='val_mse')\n",
        "# plt.title('Training and validation mean squared error')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Mean Squared Error')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "dIEMawAeF3FO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}